"""
Medical RAG System Core Implementation
åŒ»ç™‚RAGã‚·ã‚¹ãƒ†ãƒ ã®ã‚³ã‚¢å®Ÿè£…ï¼ˆæ¤œç´¢ãƒ»å›ç­”ç”Ÿæˆæ©Ÿèƒ½ï¼‰
"""

import json
import logging
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
import requests
from datetime import datetime
from pathlib import Path

from src.vector_store import MedicalVectorStore
from config.settings import settings

logger = logging.getLogger(__name__)


@dataclass
class RAGResponse:
    """RAGå›ç­”ã®ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    query: str
    answer: str
    source_documents: List[Dict]
    search_time_ms: float
    generation_time_ms: float
    total_time_ms: float
    metadata: Dict


class MedicalRAGSystem:
    """åŒ»ç™‚RAGã‚·ã‚¹ãƒ†ãƒ ã®ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        """RAGã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–"""
        self.vector_store = None
        self.ollama_base_url = settings.OLLAMA_BASE_URL
        self.ollama_model = settings.OLLAMA_MODEL
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        self.system_prompt = """ã‚ãªãŸã¯åŒ»ç™‚æ–‡çŒ®ã«åŸºã¥ã„ã¦å›ç­”ã™ã‚‹åŒ»ç™‚AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚

ã€é‡è¦ãªåˆ¶ç´„ã€‘
1. æä¾›ã•ã‚ŒãŸåŒ»å­¦æ–‡çŒ®ã®æƒ…å ±ã®ã¿ã‚’ä½¿ç”¨ã—ã¦å›ç­”ã—ã¦ãã ã•ã„
2. åŒ»å­¦çš„è¨ºæ–­ã‚„æ²»ç™‚ã®åŠ©è¨€ã¯è¡Œã‚ãšã€æ–‡çŒ®æƒ…å ±ã®è¦ç´„ã«ç•™ã‚ã¦ãã ã•ã„
3. ä¸ç¢ºå®Ÿãªæƒ…å ±ã«ã¤ã„ã¦ã¯æ˜ç¢ºã«ã€Œæ–‡çŒ®ã§ã¯è¨€åŠã•ã‚Œã¦ã„ã¾ã›ã‚“ã€ã¨è¿°ã¹ã¦ãã ã•ã„
4. å›ç­”ã®æœ€å¾Œã«å‚è€ƒæ–‡çŒ®ã®PMIDã‚’å¿…ãšè¨˜è¼‰ã—ã¦ãã ã•ã„

ã€å›ç­”å½¢å¼ã€‘
- ç°¡æ½”ã§åˆ†ã‹ã‚Šã‚„ã™ã„æ—¥æœ¬èªã§å›ç­”
- æ ¹æ‹ ã¨ãªã‚‹æ–‡çŒ®æƒ…å ±ã‚’æ˜ç¤º
- åŒ»ç™‚å¾“äº‹è€…ã¸ã®ç›¸è«‡ã‚’æ¨å¥¨ã™ã‚‹æ–‡è¨€ã‚’å«ã‚ã‚‹

ä»¥ä¸‹ã®åŒ»å­¦æ–‡çŒ®ã‚’å‚è€ƒã«ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å›ç­”ã—ã¦ãã ã•ã„ï¼š

{context}

è³ªå•: {question}

å›ç­”:"""
        
        self._initialize_components()
    
    def _initialize_components(self):
        """ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®åˆæœŸåŒ–"""
        try:
            # ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®åˆæœŸåŒ–
            self.vector_store = MedicalVectorStore()
            logger.info("RAG system initialized successfully")
        except Exception as e:
            logger.error(f"Failed to initialize RAG system: {e}")
            raise
    
    def translate_query_to_english(self, query: str) -> Tuple[str, float]:
        """
        æ—¥æœ¬èªã‚¯ã‚¨ãƒªã‚’è‹±èªã«ç¿»è¨³
        
        Args:
            query: æ—¥æœ¬èªã®æ¤œç´¢ã‚¯ã‚¨ãƒª
            
        Returns:
            (è‹±è¨³ã‚¯ã‚¨ãƒª, ç¿»è¨³æ™‚é–“(ms))
        """
        start_time = datetime.now()
        
        try:
            # Ollamaã§åŒ»å­¦å°‚é–€ç”¨èªã‚’è€ƒæ…®ã—ãŸç¿»è¨³
            translation_prompt = f"""Translate this Japanese medical query to English. Use precise medical terminology. Give only the English translation, no explanations.

Japanese: {query}
English:"""
            
            response = requests.post(
                f"{self.ollama_base_url}/api/generate",
                json={
                    "model": self.ollama_model,
                    "prompt": translation_prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.1,
                        "num_predict": 50,
                    }
                },
                timeout=15
            )
            
            response.raise_for_status()
            result = response.json()
            
            english_query = result.get('response', '').strip()
            
            # ç¿»è¨³çµæœã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
            english_query = english_query.replace('\n', ' ').strip()
            
            # å†—é•·ãªèª¬æ˜ã‚’å‰Šé™¤ï¼ˆæœ€åˆã®æ–‡ã®ã¿å–å¾—ï¼‰
            if '.' in english_query:
                english_query = english_query.split('.')[0].strip()
            
            if not english_query:
                english_query = query  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            
            translation_time = (datetime.now() - start_time).total_seconds() * 1000
            logger.info(f"Translated '{query}' -> '{english_query}' ({translation_time:.2f}ms)")
            
            return english_query, translation_time
            
        except Exception as e:
            logger.error(f"Translation failed: {e}")
            translation_time = (datetime.now() - start_time).total_seconds() * 1000
            return query, translation_time  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

    def search_relevant_documents(self, 
                                query: str, 
                                top_k: int = None,
                                similarity_threshold: float = None) -> Tuple[List[Dict], float, str]:
        """
        é–¢é€£æ–‡æ›¸ã®æ¤œç´¢ï¼ˆç¿»è¨³å¯¾å¿œï¼‰
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            top_k: å–å¾—ã™ã‚‹æ–‡æ›¸æ•°
            similarity_threshold: é¡ä¼¼åº¦é–¾å€¤
            
        Returns:
            (é–¢é€£æ–‡æ›¸ãƒªã‚¹ãƒˆ, æ¤œç´¢æ™‚é–“(ms), ä½¿ç”¨ã•ã‚ŒãŸã‚¯ã‚¨ãƒª)
        """
        start_time = datetime.now()
        
        try:
            top_k = top_k or settings.SEARCH_TOP_K
            similarity_threshold = similarity_threshold or settings.SIMILARITY_THRESHOLD
            
            # æ—¥æœ¬èªã‚¯ã‚¨ãƒªã‚’è‹±èªã«ç¿»è¨³
            english_query, translation_time = self.translate_query_to_english(query)
            
            # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢å®Ÿè¡Œ
            search_results = self.vector_store.search_similar(
                query=english_query,
                top_k=top_k * 2  # é–¾å€¤ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ç”¨ã«å¤šã‚ã«å–å¾—
            )
            
            # é¡ä¼¼åº¦é–¾å€¤ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            filtered_results = [
                result for result in search_results 
                if result['similarity_score'] >= similarity_threshold
            ]
            
            # ä¸Šä½kä»¶ã«åˆ¶é™
            final_results = filtered_results[:top_k]
            
            search_time = (datetime.now() - start_time).total_seconds() * 1000
            
            logger.info(f"Found {len(final_results)} relevant documents (total: {search_time:.2f}ms, translation: {translation_time:.2f}ms)")
            return final_results, search_time, english_query
            
        except Exception as e:
            logger.error(f"Document search failed: {e}")
            return [], 0.0, query
    
    def _format_context_from_documents(self, documents: List[Dict]) -> str:
        """
        æ–‡æ›¸ãƒªã‚¹ãƒˆã‹ã‚‰ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ–‡å­—åˆ—ã‚’ç”Ÿæˆ
        
        Args:
            documents: æ¤œç´¢çµæœæ–‡æ›¸ãƒªã‚¹ãƒˆ
            
        Returns:
            ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ¸ˆã¿ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
        """
        if not documents:
            return "é–¢é€£ã™ã‚‹åŒ»å­¦æ–‡çŒ®ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        
        context_parts = []
        
        for i, doc in enumerate(documents, 1):
            metadata = doc['metadata']
            content = doc['content']
            
            # æ–‡æ›¸æƒ…å ±ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
            doc_info = f"ã€æ–‡çŒ®{i}ã€‘"
            if metadata.get('title'):
                doc_info += f" {metadata['title']}"
            if metadata.get('authors'):
                authors = metadata['authors'][:2] if isinstance(metadata['authors'], list) else []
                if authors:
                    doc_info += f" (è‘—è€…: {', '.join(authors)})"
            if metadata.get('journal'):
                doc_info += f" - {metadata['journal']}"
            if metadata.get('publication_date'):
                doc_info += f" ({metadata['publication_date']})"
            if metadata.get('pmid'):
                doc_info += f" [PMID: {metadata['pmid']}]"
            
            doc_info += f"\né¡ä¼¼åº¦: {doc['similarity_score']:.3f}\n"
            doc_info += f"å†…å®¹: {content}\n"
            
            context_parts.append(doc_info)
        
        return "\n" + "="*80 + "\n".join(context_parts)
    
    def generate_answer(self, query: str, context: str) -> Tuple[str, float]:
        """
        Ollamaã‚’ä½¿ç”¨ã—ãŸå›ç­”ç”Ÿæˆ
        
        Args:
            query: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•
            context: æ¤œç´¢ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            (ç”Ÿæˆã•ã‚ŒãŸå›ç­”, ç”Ÿæˆæ™‚é–“(ms))
        """
        start_time = datetime.now()
        
        try:
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
            prompt = self.system_prompt.format(
                context=context,
                question=query
            )
            
            # Ollama APIã¸ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
            response = requests.post(
                f"{self.ollama_base_url}/api/generate",
                json={
                    "model": self.ollama_model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.1,  # ä¸€è²«æ€§é‡è¦–
                        "top_p": 0.9,
                        "num_predict": 1000,  # æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
                    }
                },
                timeout=settings.GENERATION_TIMEOUT
            )
            
            response.raise_for_status()
            result = response.json()
            
            answer = result.get('response', 'å›ç­”ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚')
            generation_time = (datetime.now() - start_time).total_seconds() * 1000
            
            logger.info(f"Answer generated (generation: {generation_time:.2f}ms)")
            return answer, generation_time
            
        except requests.exceptions.Timeout:
            logger.error("Answer generation timed out")
            return "å›ç­”ç”ŸæˆãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸã€‚ã‚ˆã‚Šç°¡æ½”ãªè³ªå•ã§å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚", 0.0
        except requests.exceptions.ConnectionError:
            logger.error("Failed to connect to Ollama server")
            return "Ollamaã‚µãƒ¼ãƒãƒ¼ã«æ¥ç¶šã§ãã¾ã›ã‚“ã€‚ã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚", 0.0
        except Exception as e:
            logger.error(f"Answer generation failed: {e}")
            return f"å›ç­”ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}", 0.0
    
    def query(self, 
             user_query: str,
             top_k: int = None,
             similarity_threshold: float = None) -> RAGResponse:
        """
        RAGã‚¯ã‚¨ãƒªã®å®Ÿè¡Œï¼ˆæ¤œç´¢ + ç”Ÿæˆï¼‰
        
        Args:
            user_query: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•
            top_k: æ¤œç´¢ã™ã‚‹æ–‡æ›¸æ•°
            similarity_threshold: é¡ä¼¼åº¦é–¾å€¤
            
        Returns:
            RAGå›ç­”ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        """
        total_start_time = datetime.now()
        
        logger.info(f"Processing RAG query: {user_query}")
        
        # 1. é–¢é€£æ–‡æ›¸æ¤œç´¢
        relevant_docs, search_time, english_query = self.search_relevant_documents(
            query=user_query,
            top_k=top_k,
            similarity_threshold=similarity_threshold
        )
        
        if not relevant_docs:
            # é–¢é€£æ–‡æ›¸ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆ
            total_time = (datetime.now() - total_start_time).total_seconds() * 1000
            return RAGResponse(
                query=user_query,
                answer="ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€ã”è³ªå•ã«é–¢é€£ã™ã‚‹åŒ»å­¦æ–‡çŒ®ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ç•°ãªã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚",
                source_documents=[],
                search_time_ms=search_time,
                generation_time_ms=0.0,
                total_time_ms=total_time,
                metadata={
                    'similarity_threshold': similarity_threshold or settings.SIMILARITY_THRESHOLD,
                    'requested_top_k': top_k or settings.SEARCH_TOP_K,
                    'found_documents': 0,
                    'english_query': english_query
                }
            )
        
        # 2. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰
        context = self._format_context_from_documents(relevant_docs)
        
        # 3. å›ç­”ç”Ÿæˆ
        answer, generation_time = self.generate_answer(user_query, context)
        
        # 4. çµæœæ§‹ç¯‰
        total_time = (datetime.now() - total_start_time).total_seconds() * 1000
        
        return RAGResponse(
            query=user_query,
            answer=answer,
            source_documents=relevant_docs,
            search_time_ms=search_time,
            generation_time_ms=generation_time,
            total_time_ms=total_time,
            metadata={
                'similarity_threshold': similarity_threshold or settings.SIMILARITY_THRESHOLD,
                'requested_top_k': top_k or settings.SEARCH_TOP_K,
                'found_documents': len(relevant_docs),
                'ollama_model': self.ollama_model,
                'english_query': english_query
            }
        )
    
    def get_system_status(self) -> Dict:
        """
        ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ã®ç¢ºèª
        
        Returns:
            ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹æƒ…å ±
        """
        status = {
            'vector_store': False,
            'ollama_server': False,
            'total_documents': 0,
            'available_models': []
        }
        
        try:
            # ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢çŠ¶æ…‹ç¢ºèª
            if self.vector_store:
                stats = self.vector_store.get_collection_stats()
                status['vector_store'] = True
                status['total_documents'] = stats.get('total_documents', 0)
        except Exception as e:
            logger.error(f"Vector store check failed: {e}")
        
        try:
            # Ollama ã‚µãƒ¼ãƒãƒ¼çŠ¶æ…‹ç¢ºèª
            response = requests.get(f"{self.ollama_base_url}/api/tags", timeout=5)
            if response.status_code == 200:
                status['ollama_server'] = True
                models_data = response.json()
                status['available_models'] = [model['name'] for model in models_data.get('models', [])]
        except Exception as e:
            logger.error(f"Ollama server check failed: {e}")
        
        return status


def main():
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    # ãƒ­ã‚°è¨­å®š
    logging.basicConfig(level=logging.INFO)
    
    # RAGã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    rag_system = MedicalRAGSystem()
    
    # ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ç¢ºèª
    status = rag_system.get_system_status()
    print("ğŸ” Medical RAG System Status")
    print("=" * 40)
    print(f"Vector Store: {'âœ…' if status['vector_store'] else 'âŒ'}")
    print(f"Ollama Server: {'âœ…' if status['ollama_server'] else 'âŒ'}")
    print(f"Total Documents: {status['total_documents']}")
    print(f"Available Models: {status['available_models']}")
    
    if not (status['vector_store'] and status['ollama_server']):
        print("\nâŒ System not ready. Check vector store and Ollama server.")
        return
    
    # ãƒ†ã‚¹ãƒˆã‚¯ã‚¨ãƒªå®Ÿè¡Œ
    test_queries = [
        "COVID-19ã®æ²»ç™‚æ³•ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„",
        "AIã«ã‚ˆã‚‹åŒ»ç™‚è¨ºæ–­ã®ç²¾åº¦ã¯ã©ã®ç¨‹åº¦ã§ã™ã‹ï¼Ÿ",
        "ãŒã‚“å…ç–«ç™‚æ³•ã®æœ€æ–°ã®ç ”ç©¶æˆæœã¯ï¼Ÿ"
    ]
    
    print(f"\nğŸ§ª Running Test Queries")
    print("=" * 50)
    
    for i, query in enumerate(test_queries, 1):
        print(f"\nã€ã‚¯ã‚¨ãƒª {i}ã€‘{query}")
        print("-" * 50)
        
        response = rag_system.query(query)
        
        print(f"å›ç­”: {response.answer[:200]}...")
        print(f"æ¤œç´¢æ™‚é–“: {response.search_time_ms:.2f}ms")
        print(f"ç”Ÿæˆæ™‚é–“: {response.generation_time_ms:.2f}ms")
        print(f"ç·æ™‚é–“: {response.total_time_ms:.2f}ms")
        print(f"å‚è€ƒæ–‡çŒ®æ•°: {len(response.source_documents)}")
        
        if response.source_documents:
            print("å‚è€ƒæ–‡çŒ®:")
            for j, doc in enumerate(response.source_documents[:2], 1):
                metadata = doc['metadata']
                title = metadata.get('title', 'Unknown')[:50]
                pmid = metadata.get('pmid', 'Unknown')
                score = doc['similarity_score']
                print(f"  {j}. {title}... [PMID: {pmid}] (é¡ä¼¼åº¦: {score:.3f})")
    
    print(f"\nâœ… RAG System test completed!")


if __name__ == "__main__":
    main()
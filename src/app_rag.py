"""
Medical RAG System - Streamlit Application
åŒ»ç™‚RAGã‚·ã‚¹ãƒ†ãƒ çµ±åˆStreamlitã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
"""

import streamlit as st
import time
import json
from datetime import datetime
from typing import Dict, List, Optional
import logging
import requests

import sys
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.rag_system import MedicalRAGSystem, RAGResponse
from config.settings import settings

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="Medical RAG System",
    page_icon="ğŸ¥",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
if 'rag_system' not in st.session_state:
    st.session_state.rag_system = None
if 'query_history' not in st.session_state:
    st.session_state.query_history = []
if 'system_initialized' not in st.session_state:
    st.session_state.system_initialized = False
if 'selected_query' not in st.session_state:
    st.session_state.selected_query = ""
if 'query_input' not in st.session_state:
    st.session_state.query_input = ""
if 'rag_response' not in st.session_state:
    st.session_state.rag_response = None
if 'llm_response' not in st.session_state:
    st.session_state.llm_response = None

def initialize_rag_system():
    """RAGã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–"""
    try:
        with st.spinner("RAGã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–ã—ã¦ã„ã¾ã™..."):
            rag_system = MedicalRAGSystem()
            status = rag_system.get_system_status()
            
            if status['vector_store'] and status['ollama_server']:
                st.session_state.rag_system = rag_system
                st.session_state.system_initialized = True
                st.success("âœ… RAGã‚·ã‚¹ãƒ†ãƒ ãŒæ­£å¸¸ã«åˆæœŸåŒ–ã•ã‚Œã¾ã—ãŸ")
                return status
            else:
                st.error("âŒ ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
                return status
                
    except Exception as e:
        st.error(f"âŒ åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return None

def display_system_status(status: Dict):
    """ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ã®è¡¨ç¤º"""
    st.sidebar.markdown("### ğŸ” ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹")
    
    col1, col2 = st.sidebar.columns(2)
    
    with col1:
        st.metric(
            "ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢",
            "âœ…" if status['vector_store'] else "âŒ",
            delta=None
        )
        
    with col2:
        st.metric(
            "Ollamaã‚µãƒ¼ãƒãƒ¼", 
            "âœ…" if status['ollama_server'] else "âŒ",
            delta=None
        )
    
    st.sidebar.metric("æ–‡æ›¸æ•°", f"{status['total_documents']:,}ä»¶")
    
    if status['available_models']:
        st.sidebar.markdown("**åˆ©ç”¨å¯èƒ½ãƒ¢ãƒ‡ãƒ«:**")
        for model in status['available_models']:
            st.sidebar.text(f"â€¢ {model}")

def display_search_settings():
    """æ¤œç´¢è¨­å®šã®è¡¨ç¤º"""
    st.sidebar.markdown("### âš™ï¸ æ¤œç´¢è¨­å®š")
    
    # æ¤œç´¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
    top_k = st.sidebar.slider(
        "å–å¾—æ–‡æ›¸æ•°",
        min_value=1,
        max_value=10,
        value=settings.SEARCH_TOP_K,
        help="æ¤œç´¢ã§å–å¾—ã™ã‚‹é–¢é€£æ–‡æ›¸ã®æœ€å¤§æ•°"
    )
    
    similarity_threshold = st.sidebar.slider(
        "é¡ä¼¼åº¦é–¾å€¤",
        min_value=0.0,
        max_value=1.0,
        value=settings.SIMILARITY_THRESHOLD,
        step=0.05,
        help="æ–‡æ›¸ã‚’é–¢é€£ã‚ã‚Šã¨åˆ¤å®šã™ã‚‹æœ€å°é¡ä¼¼åº¦"
    )
    
    return top_k, similarity_threshold

def display_query_examples():
    """ã‚¯ã‚¨ãƒªä¾‹ã®è¡¨ç¤º"""
    st.sidebar.markdown("### ğŸ’¡ è³ªå•ä¾‹")
    
    examples = [
        "COVID-19ã®æ²»ç™‚æ³•ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„",
        "AIã«ã‚ˆã‚‹åŒ»ç™‚è¨ºæ–­ã®ç²¾åº¦ã¯ã©ã®ç¨‹åº¦ã§ã™ã‹ï¼Ÿ",
        "ãŒã‚“å…ç–«ç™‚æ³•ã®æœ€æ–°ã®ç ”ç©¶æˆæœã¯ï¼Ÿ",
        "é éš”åŒ»ç™‚ã®åŠ¹æœã¨èª²é¡Œã«ã¤ã„ã¦",
        "æ©Ÿæ¢°å­¦ç¿’ã‚’ç”¨ã„ãŸå‰µè–¬ç ”ç©¶ã®ç¾çŠ¶"
    ]
    
    for i, example in enumerate(examples):
        if st.sidebar.button(f"ğŸ“ {example[:20]}...", key=f"example_{i}"):
            st.session_state.selected_query = example
            st.rerun()

def format_response_display(response: RAGResponse, response_type: str = "RAG"):
    """ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®è¡¨ç¤ºãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    
    # åŸºæœ¬æƒ…å ±
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("æ¤œç´¢æ™‚é–“", f"{response.search_time_ms:.0f}ms" if hasattr(response, 'search_time_ms') else "N/A")
    with col2:
        st.metric("ç”Ÿæˆæ™‚é–“", f"{response.generation_time_ms:.0f}ms" if hasattr(response, 'generation_time_ms') else "N/A")
    with col3:
        if response_type == "RAG" and hasattr(response, 'source_documents'):
            st.metric("å‚è€ƒæ–‡çŒ®æ•°", len(response.source_documents))
        else:
            st.metric("å‚è€ƒæ–‡çŒ®æ•°", "N/A")
    
    # è‹±è¨³ã‚¯ã‚¨ãƒªè¡¨ç¤º
    if hasattr(response, 'metadata') and response.metadata and 'english_query' in response.metadata:
        st.info(f"ğŸ”„ **æ¤œç´¢ã«ä½¿ç”¨ã•ã‚ŒãŸè‹±è¨³**: {response.metadata['english_query']}")
    
    # å›ç­”è¡¨ç¤º
    st.markdown("### ğŸ’¬ å›ç­”")
    if hasattr(response, 'answer'):
        st.markdown(response.answer)
    else:
        st.markdown(response)
    
    # å‚è€ƒæ–‡çŒ®è¡¨ç¤ºï¼ˆRAGã®ã¿ï¼‰
    if response_type == "RAG" and hasattr(response, 'source_documents') and response.source_documents:
        st.markdown("### ğŸ“š å‚è€ƒæ–‡çŒ®")
        
        for i, doc in enumerate(response.source_documents, 1):
            with st.expander(f"ğŸ“„ æ–‡çŒ® {i} (é¡ä¼¼åº¦: {doc['similarity_score']:.3f})"):
                metadata = doc['metadata']
                
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¡¨ç¤º
                col1, col2 = st.columns([2, 1])
                
                with col1:
                    st.markdown(f"**ã‚¿ã‚¤ãƒˆãƒ«**: {metadata.get('title', 'N/A')}")
                    if metadata.get('authors'):
                        authors = metadata['authors'][:3] if isinstance(metadata['authors'], list) else []
                        if authors:
                            st.markdown(f"**è‘—è€…**: {', '.join(authors)}")
                    st.markdown(f"**ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«**: {metadata.get('journal', 'N/A')}")
                
                with col2:
                    st.markdown(f"**ç™ºè¡Œæ—¥**: {metadata.get('publication_date', 'N/A')}")
                    if metadata.get('pmid'):
                        pmid_url = f"https://pubmed.ncbi.nlm.nih.gov/{metadata['pmid']}"
                        st.markdown(f"**PMID**: [{metadata['pmid']}]({pmid_url})")
                    if metadata.get('doi'):
                        doi_url = f"https://doi.org/{metadata['doi']}"
                        st.markdown(f"**DOI**: [{metadata['doi']}]({doi_url})")
                
                # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„è¡¨ç¤º
                st.markdown("**å†…å®¹æŠœç²‹**:")
                st.text(doc['content'][:500] + "..." if len(doc['content']) > 500 else doc['content'])
                
                # MeSHç”¨èªã¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
                if metadata.get('mesh_terms'):
                    mesh_terms = metadata['mesh_terms'][:5] if isinstance(metadata['mesh_terms'], list) else []
                    if mesh_terms:
                        st.markdown(f"**MeSHç”¨èª**: {', '.join(mesh_terms)}")
                
                if metadata.get('keywords'):
                    keywords = metadata['keywords'][:5] if isinstance(metadata['keywords'], list) else []
                    if keywords:
                        st.markdown(f"**ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰**: {', '.join(keywords)}")

def display_query_history():
    """ã‚¯ã‚¨ãƒªå±¥æ­´ã®è¡¨ç¤º"""
    if st.session_state.query_history:
        st.sidebar.markdown("### ğŸ“ æœ€è¿‘ã®è³ªå•")
        
        for i, (query, timestamp) in enumerate(reversed(st.session_state.query_history[-5:])):
            time_str = timestamp.strftime("%H:%M")
            if st.sidebar.button(f"{time_str} {query[:15]}...", key=f"history_{i}"):
                st.session_state.selected_query = query
                st.rerun()

def save_query_to_history(query: str, response=None):
    """ã‚¯ã‚¨ãƒªå±¥æ­´ã¸ã®ä¿å­˜"""
    timestamp = datetime.now()
    st.session_state.query_history.append((query, timestamp))
    
    # å±¥æ­´ã®åˆ¶é™ï¼ˆæœ€å¤§50ä»¶ï¼‰
    if len(st.session_state.query_history) > 50:
        st.session_state.query_history = st.session_state.query_history[-50:]

def display_medical_disclaimer():
    """åŒ»ç™‚å…è²¬äº‹é …ã®è¡¨ç¤º"""
    st.sidebar.markdown("---")
    st.sidebar.markdown("### âš ï¸ é‡è¦ãªæ³¨æ„äº‹é …")
    st.sidebar.warning(
        "ã“ã®ã‚·ã‚¹ãƒ†ãƒ ã¯ç ”ç©¶ãƒ»æ•™è‚²ç›®çš„ã®ã‚‚ã®ã§ã™ã€‚\n\n"
        "åŒ»å­¦çš„ãªè¨ºæ–­ã‚„æ²»ç™‚ã®åŠ©è¨€ã‚’è¡Œã†ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚\n\n"
        "åŒ»ç™‚ã«é–¢ã™ã‚‹åˆ¤æ–­ã¯å¿…ãšåŒ»ç™‚å¾“äº‹è€…ã«ã”ç›¸è«‡ãã ã•ã„ã€‚"
    )

class LLMResponse:
    """ç›´æ¥LLMå›ç­”ã®ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    def __init__(self, query: str, answer: str, generation_time_ms: float):
        self.query = query
        self.answer = answer
        self.generation_time_ms = generation_time_ms
        self.search_time_ms = 0.0  # LLMã¯æ¤œç´¢ã—ãªã„ãŸã‚
        self.source_documents = []  # LLMã¯å‚è€ƒæ–‡çŒ®ãªã—
        self.metadata = {}

def get_llm_response(query: str) -> LLMResponse:
    """ç›´æ¥LLMå›ç­”ã‚’å–å¾—"""
    start_time = datetime.now()
    
    try:
        # åŒ»ç™‚å°‚ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        medical_prompt = f"""ã‚ãªãŸã¯åŒ»ç™‚çŸ¥è­˜ã«ç‰¹åŒ–ã—ãŸAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚

ã€é‡è¦ãªåˆ¶ç´„ã€‘
1. åŒ»å­¦çš„è¨ºæ–­ã‚„æ²»ç™‚ã®åŠ©è¨€ã¯è¡Œã‚ãšã€ä¸€èˆ¬çš„ãªåŒ»ç™‚æƒ…å ±ã®æä¾›ã«ç•™ã‚ã¦ãã ã•ã„
2. ä¸ç¢ºå®Ÿãªæƒ…å ±ã«ã¤ã„ã¦ã¯æ˜ç¢ºã«ã€Œä¸€èˆ¬çš„ãªæƒ…å ±ã§ã™ã€ã¨è¿°ã¹ã¦ãã ã•ã„
3. å›ç­”ã®æœ€å¾Œã«åŒ»ç™‚å¾“äº‹è€…ã¸ã®ç›¸è«‡ã‚’æ¨å¥¨ã™ã‚‹æ–‡è¨€ã‚’å«ã‚ã¦ãã ã•ã„

ã€å›ç­”å½¢å¼ã€‘
- ç°¡æ½”ã§åˆ†ã‹ã‚Šã‚„ã™ã„æ—¥æœ¬èªã§å›ç­”
- ä¸€èˆ¬çš„ãªåŒ»ç™‚çŸ¥è­˜ã‚’åŸºã«ã—ãŸæƒ…å ±æä¾›
- æœ€æ–°ã®ç ”ç©¶çµæœã‚„æ–‡çŒ®æƒ…å ±ã¯ä¸æ˜ã§ã‚ã‚‹ã“ã¨ã‚’æ˜ç¤º

è³ªå•: {query}

å›ç­”:"""
        
        # Ollama APIã¸ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
        response = requests.post(
            f"{settings.OLLAMA_BASE_URL}/api/generate",
            json={
                "model": settings.OLLAMA_MODEL,
                "prompt": medical_prompt,
                "stream": False,
                "options": {
                    "temperature": 0.3,  # å°‘ã—é«˜ã‚ã®åˆæœŸå€¤ã§è‡ªç„¶ãªå›ç­”
                    "top_p": 0.9,
                    "num_predict": 1000,  # æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
                }
            },
            timeout=settings.GENERATION_TIMEOUT
        )
        
        response.raise_for_status()
        result = response.json()
        
        answer = result.get('response', 'å›ç­”ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚')
        generation_time = (datetime.now() - start_time).total_seconds() * 1000
        
        logger.info(f"LLM direct response generated ({generation_time:.2f}ms)")
        return LLMResponse(query, answer, generation_time)
        
    except requests.exceptions.Timeout:
        logger.error("LLM response generation timed out")
        return LLMResponse(query, "å›ç­”ç”ŸæˆãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸã€‚ã‚ˆã‚Šç°¡æ½”ãªè³ªå•ã§å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚", 0.0)
    except requests.exceptions.ConnectionError:
        logger.error("Failed to connect to Ollama server for LLM response")
        return LLMResponse(query, "Ollamaã‚µãƒ¼ãƒãƒ¼ã«æ¥ç¶šã§ãã¾ã›ã‚“ã€‚ã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚", 0.0)
    except Exception as e:
        logger.error(f"LLM response generation failed: {e}")
        return LLMResponse(query, f"å›ç­”ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}", 0.0)

def display_comparison_view(user_query: str, top_k: int, similarity_threshold: float):
    """æ¯”è¼ƒè¡¨ç¤ºãƒ“ãƒ¥ãƒ¼"""
    st.markdown("### ğŸ” ã‚¯ã‚¨ãƒªå®Ÿè¡Œ")
    
    # ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ã®æœ‰ç„¡ã§ãƒœã‚¿ãƒ³ã®æ´»æ€§åŒ–ã‚’åˆ¶å¾¡
    has_query = bool(user_query.strip())
    
    # ãƒœã‚¿ãƒ³ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ
    col1, col2, col3 = st.columns([1, 1, 2])
    
    with col1:
        rag_button = st.button("ğŸ” RAGå›ç­”", type="primary", disabled=not has_query)
    
    with col2:
        llm_button = st.button("ğŸ¤– LLMå›ç­”", type="secondary", disabled=not has_query)
    
    with col3:
        both_button = st.button("ğŸ”„ ä¸¡æ–¹åŒæ™‚å®Ÿè¡Œ", type="secondary", disabled=not has_query)
    
    # ã‚¯ã‚¨ãƒªå®Ÿè¡Œ
    if has_query:
        if rag_button or both_button:
            execute_rag_query(user_query, top_k, similarity_threshold)
        
        if llm_button or both_button:
            execute_llm_query(user_query)
    
    # çµæœè¡¨ç¤º
    if st.session_state.rag_response or st.session_state.llm_response:
        st.markdown("---")
        st.markdown(f"### ğŸ“‹ è³ªå•: {user_query}")
        
        # ã‚¿ãƒ–è¡¨ç¤º
        tab1, tab2, tab3 = st.tabs(["ğŸ” RAGå›ç­”", "ğŸ¤– LLMå›ç­”", "ğŸ”„ æ¯”è¼ƒ"])
        
        with tab1:
            if st.session_state.rag_response:
                format_response_display(st.session_state.rag_response, "RAG")
            else:
                st.info("RAGå›ç­”ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
        
        with tab2:
            if st.session_state.llm_response:
                format_response_display(st.session_state.llm_response, "LLM")
            else:
                st.info("LLMå›ç­”ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
        
        with tab3:
            if st.session_state.rag_response and st.session_state.llm_response:
                display_comparison_results()
            else:
                st.info("æ¯”è¼ƒã«ã¯ä¸¡æ–¹ã®å›ç­”ãŒå¿…è¦ã§ã™")

def execute_rag_query(user_query: str, top_k: int, similarity_threshold: float):
    """RAGã‚¯ã‚¨ãƒªå®Ÿè¡Œ"""
    with st.spinner("ğŸ” RAGå›ç­”ã‚’ç”Ÿæˆä¸­..."):
        try:
            response = st.session_state.rag_system.query(
                user_query=user_query.strip(),
                top_k=top_k,
                similarity_threshold=similarity_threshold
            )
            st.session_state.rag_response = response
            st.success(f"âœ… RAGå›ç­”ãŒå®Œäº†ã—ã¾ã—ãŸ ({response.generation_time_ms:.0f}ms)")
        except Exception as e:
            st.error(f"âŒ RAGã‚¨ãƒ©ãƒ¼: {str(e)}")
            logger.error(f"RAG query execution failed: {e}")

def execute_llm_query(user_query: str):
    """ç›´æ¥LLMã‚¯ã‚¨ãƒªå®Ÿè¡Œ"""
    with st.spinner("ğŸ¤– LLMå›ç­”ã‚’ç”Ÿæˆä¸­..."):
        try:
            response = get_llm_response(user_query)
            st.session_state.llm_response = response
            # æˆåŠŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«ç”Ÿæˆæ™‚é–“ã‚’è¡¨ç¤º
            st.success(f"âœ… LLMå›ç­”ãŒå®Œäº†ã—ã¾ã—ãŸ ({response.generation_time_ms:.0f}ms)")
        except Exception as e:
            st.error(f"âŒ LLMã‚¨ãƒ©ãƒ¼: {str(e)}")
            logger.error(f"LLM query execution failed: {e}")

def display_comparison_results():
    """æ¯”è¼ƒçµæœè¡¨ç¤º"""
    st.markdown("### ğŸ”„ æ¯”è¼ƒçµæœ")
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("#### ğŸ” RAGå›ç­”")
        if hasattr(st.session_state.rag_response, 'generation_time_ms'):
            st.metric("ç”Ÿæˆæ™‚é–“", f"{st.session_state.rag_response.generation_time_ms:.0f}ms")
        if hasattr(st.session_state.rag_response, 'source_documents'):
            st.metric("å‚è€ƒæ–‡çŒ®æ•°", len(st.session_state.rag_response.source_documents))
    
        # å›ç­”è¡¨ç¤º
        st.markdown("### ğŸ’¬ å›ç­”")
        if hasattr(st.session_state.rag_response, 'answer'):
            st.markdown(st.session_state.rag_response.answer)
        else:
            st.markdown(st.session_state.rag_response)
    
    with col2:
        st.markdown("#### ğŸ¤– LLMå›ç­”")
        if hasattr(st.session_state.llm_response, 'generation_time_ms'):
            st.metric("ç”Ÿæˆæ™‚é–“", f"{st.session_state.llm_response.generation_time_ms:.0f}ms")
        else:
            st.metric("ç”Ÿæˆæ™‚é–“", "N/A")
        st.metric("å‚è€ƒæ–‡çŒ®æ•°", "0")
    
        # å›ç­”è¡¨ç¤º
        st.markdown("### ğŸ’¬ å›ç­”")
        if hasattr(st.session_state.llm_response, 'answer'):
            st.markdown(st.session_state.llm_response.answer)
        else:
            st.markdown(st.session_state.llm_response)

def main():
    """ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³"""
    
    # ãƒ˜ãƒƒãƒ€ãƒ¼
    st.title("ğŸ¥ Medical RAG System")
    st.markdown("**åŒ»ç™‚æ–‡çŒ®æ¤œç´¢ãƒ»å›ç­”ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ **")
    st.markdown("---")
    
    # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    if not st.session_state.system_initialized:
        st.markdown("### ğŸ“‹ ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–")
        
        if st.button("ğŸš€ RAGã‚·ã‚¹ãƒ†ãƒ ã‚’é–‹å§‹", type="primary"):
            status = initialize_rag_system()
            if status:
                display_system_status(status)
                st.rerun()
        
        st.info("ã€ŒRAGã‚·ã‚¹ãƒ†ãƒ ã‚’é–‹å§‹ã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–ã—ã¦ãã ã•ã„ã€‚")
        return
    
    # ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹è¡¨ç¤º
    if st.session_state.rag_system:
        status = st.session_state.rag_system.get_system_status()
        display_system_status(status)
        
        # ã‚·ã‚¹ãƒ†ãƒ ãŒæ­£å¸¸ã§ãªã„å ´åˆã®è­¦å‘Š
        if not (status['vector_store'] and status['ollama_server']):
            st.error("âš ï¸ ã‚·ã‚¹ãƒ†ãƒ ã«å•é¡ŒãŒã‚ã‚Šã¾ã™ã€‚ChromaDBã¾ãŸã¯Ollamaã‚µãƒ¼ãƒãƒ¼ã®çŠ¶æ…‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            return
    
    # æ¤œç´¢è¨­å®š
    top_k, similarity_threshold = display_search_settings()
    
    # ã‚¯ã‚¨ãƒªå±¥æ­´è¡¨ç¤º
    display_query_history()
    
    # ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢
    st.markdown("### ğŸ” åŒ»ç™‚æ–‡çŒ®æ¤œç´¢")
    
    user_query = st.text_area(
        "åŒ»ç™‚ã«é–¢ã™ã‚‹è³ªå•ã‚’æ—¥æœ¬èªã§å…¥åŠ›ã—ã¦ãã ã•ã„ï¼š",
        height=100,
        placeholder="ä¾‹ï¼šCOVID-19ã®æ²»ç™‚æ³•ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„",
    )
    
    # ã‚·ã‚¹ãƒ†ãƒ ãƒã‚§ãƒƒã‚¯
    if not st.session_state.rag_system:
        st.error("RAGã‚·ã‚¹ãƒ†ãƒ ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return
    
    # æ¯”è¼ƒãƒ¢ãƒ¼ãƒ‰ã§è¡¨ç¤ºï¼ˆå¸¸ã«è¡¨ç¤ºï¼‰
    display_comparison_view(user_query, top_k, similarity_threshold)
    
    # å±¥æ­´ã«ä¿å­˜
    if st.session_state.rag_response or st.session_state.llm_response:
        save_query_to_history(user_query)
    
    # å±¥æ­´ã‚¯ãƒªã‚¢
    if st.button("ğŸ—‘ï¸ å±¥æ­´ã‚’ã‚¯ãƒªã‚¢"):
        st.session_state.query_history = []
        st.session_state.rag_response = None
        st.session_state.llm_response = None
        st.success("å±¥æ­´ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")
    
    # å…è²¬äº‹é …
    display_medical_disclaimer()
    
    # ãƒ•ãƒƒã‚¿ãƒ¼
    st.markdown("---")
    st.markdown(
        """
        <div style='text-align: center; color: #666;'>
        <small>
        ğŸ”¬ Medical RAG System v1.0 | 
        ğŸ“š PubMedæ–‡çŒ®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹é€£æº | 
        ğŸ¤– Llama3.2-3B + ChromaDB
        </small>
        </div>
        """,
        unsafe_allow_html=True
    )

if __name__ == "__main__":
    main()